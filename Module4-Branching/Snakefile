### IMPORTS ###
import snakemake 
import os
import re

samples: list[str] = [
    "Sample1",
    "Sample2", 
    "Sample3"
]

wildcard_constraints: 
    sample = "[A-Za-z0-9]+"


rule all:
    input: 
        expand("output/data/{sample}-sorted.bai", sample = samples),
        "output/counts/percent_loss_sorted.txt"

# Uses the same input as rule fastp. 
rule count_raw:
    input:
        r1 = "../data/{sample}_R1.fastq",
        r2 = "../data/{sample}_R2.fastq"
    output:
        raw_counts = temp("output/counts/{sample}-raw-count.txt")
    threads: 
        1
    resources: 
        mem_mb = 4000
    log: 
        stderr = "logs/{sample}-raw-count.log"
    shell:
        """
        R1_COUNT=$( cat {input.r1} | wc -l )
        R2_COUNT=$( cat {input.r2} | wc -l )
        TOTAL=$( echo "($R1_COUNT + $R2_COUNT)/4" | bc )
        echo -e "Sample\tRaw_Count" > {output.raw_counts}
        echo -e "{wildcards.sample}\t$TOTAL" >> {output.raw_counts}
        """

# Uses the same input as the rule count_raw
rule fastp: 
    input:
        r1 = "../data/{sample}_R1.fastq",
        r2 = "../data/{sample}_R2.fastq"
    output:
        r1_clean = "output/data/{sample}_R1.clean.fastq",
        r2_clean = "output/data/{sample}_R2.clean.fastq",
        html_report = "output/reports/{sample}-fastp-report.html",
        json_report = "output/reports/{sample}-fastp-report.json"
    threads: 
        4
    resources:
        mem_mb = 4000
    log:
        stderr = "logs/{sample}-fastp.log"
    params:
        qual = 20
    shell:
        """
        fastp --in1 {input.r1} --in2 {input.r2} \
        --out1 {output.r1_clean} --out2 {output.r2_clean} \
        --detect_adapter_for_pe \
        --average_qual {params.qual} \
        --thread {threads} \
        --html {output.html_report} \
        --json {output.json_report} 2> {log.stderr}
        """

rule bwa_mem: 
    input:
        r1_clean = "output/data/{sample}_R1.clean.fastq",
        r2_clean = "output/data/{sample}_R2.clean.fastq",
        ref = "../reference/chr1-3.fa"
    output:
        sam = temp("output/data/{sample}-aligned.sam")
    threads: 
        4
    resources: 
        mem_mb = 4000
    log:
        stderr = "logs/{sample}-bwamem.log"
    shell: 
        """
        bwa mem {input.ref} {input.r1_clean} {input.r2_clean} -t {threads} > {output.sam} 2> {log.stderr}
        """

rule convert_sam_to_bam: 
    input:
        sam = "output/data/{sample}-aligned.sam"
    output:
        bam = temp("output/data/{sample}.bam")
    threads: 
        4
    resources: 
        mem_mb = 1000
    log:
        stderr = "logs/{sample}_convert.log"
    shell:
        """
        samtools view -F 4 --bam {input.sam} -o {output.bam} 2> {log.stderr}
        """

rule sort_bam: 
    input:
        bam = "output/data/{sample}.bam"
    output: 
        bam_sort = "output/data/{sample}-sorted.bam"
    threads: 
        4
    resources:
        mem_mb = 4000
    log:
        stderr = "output/logs/{sample}-sort.log"
    shell:
        """
        samtools sort {input.bam} -o {output.bam_sort} 2> {log.stderr}
        """

# Depends on the output of rules fastp and count_raw for the same sample. 
rule count_bam:
    input:
        bam_sort = "output/data/{sample}-sorted.bam",
        raw_counts = "output/counts/{sample}-raw-count.txt"
    output:
        proc_counts = "output/counts/{sample}-processed-counts.txt",
        merged_counts = "output/counts/{sample}-counts.txt"
    threads: 
        1
    resources:
        mem_mb = 1000
    log: 
        stderr = "output/logs/{sample}-counts.log"
    shell: 
        """
        COUNTS=$( samtools view {input.bam_sort} | wc -l ) 2> {log.stderr}
        echo "Sample\tFinal_Count" > {output.proc_counts}
        echo "{wildcards.sample}\t$COUNTS" >> {output.proc_counts}

        join -t $'\t' {input.raw_counts} {output.proc_counts} > {output.merged_counts} 2>> {log.stderr}
        """

rule index_bam: 
    input: 
        bam_sort = "output/data/{sample}-sorted.bam"
    output:
        bai = "output/data/{sample}-sorted.bai"
    threads: 
        4
    resources: 
        mem_mb = 8000
    log:
        stderr = "output/logs/{sample}-indexbam.log"
    shell:  
        """
        samtools index -b {input.bam_sort} -o {output.bai} 2> {log.stderr}
        """

# Depends on the output for count_bam for every single sample. 
rule percent_loss:
    input:
        count_files = expand("output/counts/{sample}-counts.txt", sample = samples)
    output:
        percent_loss = temp("output/counts/percent_loss.txt")
    threads: 
        1
    resources: 
        mem_mb = 4000
    log: 
        stderr = "logs/percentloss.log"
    shell:
        """
        for FILE in {input.count_files}; do
            DATA=$( tail -n 1 $FILE ) 
            START=$( tail -n 1 $FILE | cut -f2 )
            END=$( tail -n 1 $FILE | cut -f3 )
            PERC_LOSS=$( echo "( $START - $END ) / $START" | bc -l ) 2> {log.stderr}
            echo -e "${{DATA}}\t${{PERC_LOSS}}" >> {output.percent_loss} 2>> {log.stderr}
        done
        """


rule percent_loss_sorted: 
    input:
        percent_loss = "output/counts/percent_loss.txt"
    output:
        percent_loss_sorted = "output/counts/percent_loss_sorted.txt"
    threads: 
        1
    resources: 
        mem_mb = 4000
    log: 
        stderr = "logs/sortpercentloss.log"
    shell:
        """
        echo "Sample\tRaw_Count\tFinal_Count\tPercent_Loss" > {output.percent_loss_sorted} 2> {log.stderr}
        sort -k 4 -r {input.percent_loss} >> {output.percent_loss_sorted} 2>> {log.stderr}
        """